{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Project: Supply Chain Delivery Time Prediction\n",
        "\n",
        "**Business Case:** Predict real shipping time to optimize supply chain operations and identify late deliveries.\n",
        "\n",
        "**Authors:** Victoire Bourdet, Victor Barthelemy, Cyprien Lucas\n",
        "\n",
        "**Dataset:** DataCo Supply Chain Dataset (Kaggle)\n",
        "\n",
        "**GitHub Repository:** [To be added after push]"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \u26a0\ufe0f IMPORTANT: Run this cell first\n",
        "\n",
        "This cell mounts your Google Drive to access CSV files.\n",
        "\n",
        "**Required structure in your Drive:**\n",
        "```\n",
        "My Drive/\n",
        "\u251c\u2500\u2500 DataCoSupplyChainDataset.csv\n",
        "\u251c\u2500\u2500 DescriptionDataCoSupplyChain.csv\n",
        "\u2514\u2500\u2500 tokenized_access_logs.csv\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Verify files are present\n",
        "import os\n",
        "\n",
        "# Path to CSV files (directly in MyDrive)\n",
        "csv_path = '/content/drive/MyDrive/DataCoSupplyChainDataset.csv'\n",
        "\n",
        "csv_files = [\n",
        "    '/content/drive/MyDrive/DataCoSupplyChainDataset.csv',\n",
        "    '/content/drive/MyDrive/DescriptionDataCoSupplyChain.csv',\n",
        "    '/content/drive/MyDrive/tokenized_access_logs.csv'\n",
        "]\n",
        "\n",
        "print('\\n=== File Verification ===')\n",
        "all_found = True\n",
        "for file in csv_files:\n",
        "    if os.path.exists(file):\n",
        "        size = os.path.getsize(file) / (1024*1024)\n",
        "        print(f'\u2705 {os.path.basename(file)} ({size:.1f} MB)')\n",
        "    else:\n",
        "        print(f'\u274c MISSING: {file}')\n",
        "        all_found = False\n",
        "\n",
        "if all_found:\n",
        "    print('\\n\u2705 All files present. You can continue!')\n",
        "else:\n",
        "    print('\\n\u26a0\ufe0f WARNING: Some files are missing!')\n",
        "    print('Make sure files are directly in MyDrive/')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# STEP 1: Pre-Project - Business Understanding & Data Loading\n",
        "---"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Sklearn - Preprocessing\n",
        "from sklearn.model_selection import (train_test_split, GroupShuffleSplit, \n",
        "                                     cross_val_score, GridSearchCV, \n",
        "                                     RandomizedSearchCV, StratifiedKFold)\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import (OneHotEncoder, StandardScaler, \n",
        "                                   LabelEncoder, RobustScaler)\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
        "\n",
        "# Sklearn - Models\n",
        "from sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor,\n",
        "                              GradientBoostingClassifier, GradientBoostingRegressor,\n",
        "                              HistGradientBoostingRegressor, HistGradientBoostingClassifier,\n",
        "                              AdaBoostClassifier, VotingClassifier, StackingClassifier,\n",
        "                              BaggingClassifier)\n",
        "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Sklearn - Metrics\n",
        "from sklearn.metrics import (mean_absolute_error, mean_squared_error, r2_score,\n",
        "                             accuracy_score, precision_score, recall_score, \n",
        "                             f1_score, confusion_matrix, classification_report,\n",
        "                             roc_auc_score, roc_curve, ConfusionMatrixDisplay)\n",
        "\n",
        "# XGBoost (Advanced model)\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGB_AVAILABLE = True\n",
        "    print(\"\u2705 XGBoost available\")\n",
        "except ImportError:\n",
        "    !pip install xgboost -q\n",
        "    import xgboost as xgb\n",
        "    XGB_AVAILABLE = True\n",
        "    print(\"\u2705 XGBoost installed and imported\")\n",
        "\n",
        "# Display settings\n",
        "pd.set_option(\"display.max_columns\", 120)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "print(\"\\n\u2705 All libraries imported successfully!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset with encoding detection\n",
        "# csv_path is already defined from the previous cell\n",
        "\n",
        "for enc in [\"utf-8\", \"utf-8-sig\", \"latin-1\", \"cp1252\"]:\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path, encoding=enc, low_memory=False)\n",
        "        print(f\"\u2705 Loaded with encoding: {enc}\")\n",
        "        print(f\"Dataset shape: {df.shape}\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        last_err = e\n",
        "else:\n",
        "    raise last_err"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# STEP 2: Implementation of Standard Solutions\n",
        "---\n",
        "\n",
        "## 2.1 Dataset Analysis"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic dataset exploration\n",
        "print(\"=\"*60)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nShape: {df.shape[0]} rows \u00d7 {df.shape[1]} columns\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Display first rows\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FIRST 3 ROWS\")\n",
        "print(\"=\"*60)\n",
        "display(df.head(3))"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data types analysis\n",
        "print(\"=\"*60)\n",
        "print(\"DATA TYPES SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "dtypes_df = pd.DataFrame({\n",
        "    'Column': df.columns,\n",
        "    'Type': df.dtypes.values,\n",
        "    'Non-Null': df.notnull().sum().values,\n",
        "    'Null %': (df.isnull().sum() / len(df) * 100).round(2).values,\n",
        "    'Unique': df.nunique().values\n",
        "})\n",
        "display(dtypes_df)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical description\n",
        "print(\"=\"*60)\n",
        "print(\"NUMERICAL FEATURES - STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "display(df.describe().T)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing values analysis\n",
        "print(\"=\"*60)\n",
        "print(\"MISSING VALUES ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "missing = (df.isnull().sum() / len(df) * 100).round(2)\n",
        "missing = missing[missing > 0].sort_values(ascending=False)\n",
        "\n",
        "if len(missing) > 0:\n",
        "    print(f\"\\nColumns with missing values: {len(missing)}\")\n",
        "    display(missing.to_frame('Missing %'))\n",
        "    \n",
        "    # Visualize missing values\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    missing.plot(kind='bar', color='coral')\n",
        "    plt.title('Missing Values by Column (%)')\n",
        "    plt.ylabel('Percentage')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\n\u2705 No missing values in the dataset!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Data Visualization & Exploration"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Target variable analysis\n",
        "target_col = \"Days for shipping (real)\"\n",
        "scheduled_col = \"Days for shipment (scheduled)\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TARGET VARIABLE ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "y = df[target_col].dropna()\n",
        "print(\"\\nTarget statistics:\")\n",
        "print(y.describe(percentiles=[.25, .5, .75, .9, .95, .99]))\n",
        "print(f\"\\nSkewness: {y.skew():.3f}\")\n",
        "print(f\"Kurtosis: {y.kurtosis():.3f}\")\n",
        "\n",
        "# Target distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].hist(y, bins=50, color='steelblue', edgecolor='white', alpha=0.8)\n",
        "axes[0].axvline(y.mean(), color='red', linestyle='--', label=f'Mean: {y.mean():.2f}')\n",
        "axes[0].axvline(y.median(), color='orange', linestyle='--', label=f'Median: {y.median():.2f}')\n",
        "axes[0].set_title(f'Target Distribution - {target_col}')\n",
        "axes[0].set_xlabel('Days')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].boxplot(y, vert=True)\n",
        "axes[1].set_title('Target Boxplot')\n",
        "axes[1].set_ylabel('Days')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class balance analysis - Late delivery\n",
        "print(\"=\"*60)\n",
        "print(\"CLASS BALANCE ANALYSIS - LATE DELIVERY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create late delivery indicator\n",
        "df[\"is_late\"] = (df[target_col] > df[scheduled_col]).astype(int)\n",
        "\n",
        "late_counts = df[\"is_late\"].value_counts()\n",
        "late_pct = df[\"is_late\"].value_counts(normalize=True) * 100\n",
        "\n",
        "print(\"\\nLate Delivery Distribution:\")\n",
        "print(f\"  On-time (0): {late_counts[0]:,} ({late_pct[0]:.1f}%)\")\n",
        "print(f\"  Late (1):    {late_counts[1]:,} ({late_pct[1]:.1f}%)\")\n",
        "\n",
        "# Visualize class balance\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "colors = ['#2ecc71', '#e74c3c']\n",
        "axes[0].pie(late_counts, labels=['On-time', 'Late'], autopct='%1.1f%%', \n",
        "            colors=colors, explode=(0, 0.05))\n",
        "axes[0].set_title('Late Delivery Distribution')\n",
        "\n",
        "axes[1].bar(['On-time', 'Late'], late_counts.values, color=colors, edgecolor='white')\n",
        "axes[1].set_title('Late Delivery Counts')\n",
        "axes[1].set_ylabel('Count')\n",
        "for i, v in enumerate(late_counts.values):\n",
        "    axes[1].text(i, v + 1000, f'{v:,}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical features distribution\n",
        "print(\"=\"*60)\n",
        "print(\"CATEGORICAL FEATURES DISTRIBUTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "cat_cols_to_show = [\"Shipping Mode\", \"Market\", \"Order Status\", \"Type\"]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, col in enumerate(cat_cols_to_show):\n",
        "    if col in df.columns:\n",
        "        value_counts = df[col].value_counts().head(10)\n",
        "        axes[idx].barh(value_counts.index, value_counts.values, color='steelblue')\n",
        "        axes[idx].set_title(f'{col} Distribution')\n",
        "        axes[idx].set_xlabel('Count')\n",
        "        \n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Correlation Analysis"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation analysis\n",
        "print(\"=\"*60)\n",
        "print(\"CORRELATION ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define leakage patterns to exclude\n",
        "leak_patterns = [\"is_late\", \"Late_delivery_risk\", \"Delivery Status\",\n",
        "                 \"Days for shipping (real)\", \"Days for shipment (difference)\",\n",
        "                 \"Shipping Date (Actual)\", \"Delivery Date\"]\n",
        "\n",
        "def is_leak(c):\n",
        "    return any(re.search(p, c, flags=re.IGNORECASE) for p in leak_patterns)\n",
        "\n",
        "# Select numerical columns without leakage\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "num_cols_clean = [c for c in num_cols if c != target_col and not is_leak(c)]\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_cols = num_cols_clean + [target_col]\n",
        "corr_matrix = df[corr_cols].corr(method='spearman')\n",
        "\n",
        "# Correlation heatmap\n",
        "plt.figure(figsize=(14, 10))\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='RdBu_r', \n",
        "            center=0, vmin=-1, vmax=1, square=True,\n",
        "            linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Correlation Matrix (Spearman) - Numerical Features', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Top correlations with target\n",
        "print(\"\\nTop 15 correlations with target:\")\n",
        "target_corr = corr_matrix[target_col].drop(target_col).abs().sort_values(ascending=False)\n",
        "display(target_corr.head(15).to_frame('Absolute Correlation'))"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Data Pre-processing"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove non-informative columns\n",
        "print(\"=\"*60)\n",
        "print(\"DATA CLEANING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "cols_to_drop = [\n",
        "    \"Product Status\", \"Product Image\", \"Product Description\",\n",
        "    \"Customer Email\", \"Customer Fname\", \"Customer Lname\", \n",
        "    \"Customer Password\", \"Order Zipcode\"\n",
        "]\n",
        "\n",
        "cols_to_drop = [c for c in cols_to_drop if c in df.columns]\n",
        "df_clean = df.drop(columns=cols_to_drop)\n",
        "print(f\"\\nDropped columns: {cols_to_drop}\")\n",
        "print(f\"New shape: {df_clean.shape}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert date columns\n",
        "date_cols = [c for c in df_clean.columns if \"date\" in c.lower()]\n",
        "for c in date_cols:\n",
        "    df_clean[c] = pd.to_datetime(df_clean[c], errors=\"coerce\")\n",
        "\n",
        "print(f\"Converted date columns: {date_cols}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify and remove leakage columns\n",
        "print(\"=\"*60)\n",
        "print(\"LEAKAGE PREVENTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "leak_cols = [c for c in df_clean.columns if is_leak(c)]\n",
        "print(f\"\\nLeakage columns identified: {leak_cols}\")\n",
        "\n",
        "# ID columns (keep Order Id for grouping)\n",
        "id_cols = [c for c in df_clean.columns if re.search(r\"\\b(id|code)\\b\", c, flags=re.IGNORECASE)]\n",
        "keep_for_group = [\"Order Id\", \"Order Item Id\"]\n",
        "id_cols_drop = [c for c in id_cols if c not in keep_for_group]\n",
        "print(f\"ID columns to drop from features: {id_cols_drop}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare features and target\n",
        "print(\"=\"*60)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "base_exclude = set(leak_cols + [target_col] + id_cols_drop)\n",
        "X = df_clean.drop(columns=[c for c in base_exclude if c in df_clean.columns])\n",
        "\n",
        "# Remove date columns from features (keep for potential feature engineering)\n",
        "date_cols_in_X = [c for c in X.columns if \"date\" in c.lower()]\n",
        "X_wo_dates = X.drop(columns=[c for c in date_cols_in_X if c in X.columns], errors=\"ignore\")\n",
        "\n",
        "# Identify numerical and categorical features\n",
        "num_features = X_wo_dates.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_features = X_wo_dates.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "print(f\"\\nNumerical features: {len(num_features)}\")\n",
        "print(f\"Categorical features: {len(cat_features)}\")\n",
        "print(f\"\\nNum features: {num_features[:10]}...\" if len(num_features) > 10 else f\"\\nNum features: {num_features}\")\n",
        "print(f\"Cat features: {cat_features[:10]}...\" if len(cat_features) > 10 else f\"Cat features: {cat_features}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create preprocessing pipelines\n",
        "print(\"=\"*60)\n",
        "print(\"PREPROCESSING PIPELINES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Numerical pipeline: Impute + Scale\n",
        "num_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Categorical pipeline: Impute + Encode\n",
        "cat_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False, min_frequency=0.01))\n",
        "])\n",
        "\n",
        "# Combined preprocessor\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', num_transformer, num_features),\n",
        "    ('cat', cat_transformer, cat_features)\n",
        "], sparse_threshold=0.0)\n",
        "\n",
        "print(\"\u2705 Preprocessing pipelines created:\")\n",
        "print(\"   - Numerical: MedianImputer \u2192 StandardScaler\")\n",
        "print(\"   - Categorical: MostFrequentImputer \u2192 OneHotEncoder\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Test Split with Group awareness\n",
        "print(\"=\"*60)\n",
        "print(\"TRAIN-TEST SPLIT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Prepare final datasets\n",
        "feature_cols = X_wo_dates.columns.tolist()\n",
        "X_all = X_wo_dates[feature_cols].copy()\n",
        "y_reg = df_clean[target_col].copy()  # For regression\n",
        "y_class = df_clean[\"is_late\"].copy()  # For classification\n",
        "\n",
        "# Group by Order Id (multiple lines per order)\n",
        "groups = df_clean[\"Order Id\"]\n",
        "\n",
        "# Group-aware split to prevent data leakage\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, test_idx = next(gss.split(X_all, y_reg, groups))\n",
        "\n",
        "X_train, X_test = X_all.iloc[train_idx], X_all.iloc[test_idx]\n",
        "y_train_reg, y_test_reg = y_reg.iloc[train_idx], y_reg.iloc[test_idx]\n",
        "y_train_class, y_test_class = y_class.iloc[train_idx], y_class.iloc[test_idx]\n",
        "\n",
        "print(f\"\\nTraining set: {X_train.shape[0]:,} samples ({100*len(train_idx)/len(X_all):.1f}%)\")\n",
        "print(f\"Test set:     {X_test.shape[0]:,} samples ({100*len(test_idx)/len(X_all):.1f}%)\")\n",
        "print(f\"\\nClass distribution in training set:\")\n",
        "print(y_train_class.value_counts(normalize=True).round(3))"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Baseline Models Implementation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem formalization\n",
        "print(\"=\"*60)\n",
        "print(\"PROBLEM FORMALIZATION\")\n",
        "print(\"=\"*60)\n",
        "print(\"\"\"\n",
        "We address TWO complementary supervised learning problems:\n",
        "\n",
        "1. REGRESSION: Predict the real shipping time (in days)\n",
        "   - Target: y = Days for shipping (real)\n",
        "   - Metrics: MAE, RMSE, R\u00b2\n",
        "\n",
        "2. CLASSIFICATION: Predict if delivery will be late\n",
        "   - Target: is_late = 1 if (real > scheduled) else 0\n",
        "   - Metrics: Accuracy, Precision, Recall, F1-Score, ROC-AUC\n",
        "\n",
        "Evaluation protocol:\n",
        "- Group-aware split (Order Id) to prevent leakage\n",
        "- Cross-validation for model selection\n",
        "- Final evaluation on held-out test set\n",
        "\"\"\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BASELINE MODEL 1: Regression with HistGradientBoosting\n",
        "print(\"=\"*60)\n",
        "print(\"BASELINE MODEL 1: REGRESSION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create regression pipeline (optimized parameters)\n",
        "reg_model = HistGradientBoostingRegressor(\n",
        "    learning_rate=0.1,\n",
        "    max_iter=200,           # Reduced from 500\n",
        "    max_leaf_nodes=31,\n",
        "    min_samples_leaf=100,\n",
        "    l2_regularization=0.1,\n",
        "    early_stopping=True,\n",
        "    validation_fraction=0.1,\n",
        "    n_iter_no_change=10,    # Reduced from 20\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "reg_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', reg_model)\n",
        "])\n",
        "\n",
        "# Fit and predict\n",
        "print(\"Training regression model...\")\n",
        "reg_pipeline.fit(X_train, y_train_reg)\n",
        "y_pred_reg = reg_pipeline.predict(X_test)\n",
        "\n",
        "# Handle NaN values\n",
        "mask = ~np.isnan(y_test_reg.values) & ~np.isnan(y_pred_reg)\n",
        "y_true_reg_eval = y_test_reg.values[mask]\n",
        "y_pred_reg_eval = y_pred_reg[mask]\n",
        "\n",
        "# Compute metrics\n",
        "mae = mean_absolute_error(y_true_reg_eval, y_pred_reg_eval)\n",
        "rmse = np.sqrt(mean_squared_error(y_true_reg_eval, y_pred_reg_eval))\n",
        "r2 = r2_score(y_true_reg_eval, y_pred_reg_eval)\n",
        "\n",
        "print(f\"\\n\u2705 Regression Results:\")\n",
        "print(f\"   MAE:  {mae:.3f} days\")\n",
        "print(f\"   RMSE: {rmse:.3f} days\")\n",
        "print(f\"   R\u00b2:   {r2:.3f}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Residuals analysis\n",
        "print(\"=\"*60)\n",
        "print(\"RESIDUALS ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "residuals = y_pred_reg_eval - y_true_reg_eval\n",
        "p50 = np.percentile(np.abs(residuals), 50)\n",
        "p90 = np.percentile(np.abs(residuals), 90)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Residuals histogram\n",
        "axes[0].hist(residuals, bins=50, color='steelblue', edgecolor='white', alpha=0.8)\n",
        "axes[0].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero')\n",
        "axes[0].set_title(f'Residuals Distribution\\nMAE={mae:.2f} | RMSE={rmse:.2f} | P50={p50:.2f} | P90={p90:.2f}')\n",
        "axes[0].set_xlabel('Error (days)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].legend()\n",
        "\n",
        "# Predicted vs Actual\n",
        "axes[1].scatter(y_true_reg_eval, y_pred_reg_eval, alpha=0.3, s=10)\n",
        "axes[1].plot([0, 6], [0, 6], 'r--', linewidth=2, label='Perfect prediction')\n",
        "axes[1].set_title('Predicted vs Actual')\n",
        "axes[1].set_xlabel('Actual Days')\n",
        "axes[1].set_ylabel('Predicted Days')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BASELINE MODEL 2: Classification\n",
        "print(\"=\"*60)\n",
        "print(\"BASELINE MODEL 2: CLASSIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Derive late prediction from regression output\n",
        "scheduled_test = df_clean.loc[X_test.index, scheduled_col].values[mask]\n",
        "is_late_true = (y_true_reg_eval > scheduled_test).astype(int)\n",
        "is_late_pred = (y_pred_reg_eval > scheduled_test).astype(int)\n",
        "\n",
        "# Compute classification metrics\n",
        "accuracy = accuracy_score(is_late_true, is_late_pred)\n",
        "precision = precision_score(is_late_true, is_late_pred)\n",
        "recall = recall_score(is_late_true, is_late_pred)\n",
        "f1 = f1_score(is_late_true, is_late_pred)\n",
        "\n",
        "print(f\"\\n\u2705 Classification Results (from regression):\")\n",
        "print(f\"   Accuracy:  {accuracy:.3f}\")\n",
        "print(f\"   Precision: {precision:.3f}\")\n",
        "print(f\"   Recall:    {recall:.3f}\")\n",
        "print(f\"   F1-Score:  {f1:.3f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(is_late_true, is_late_pred, target_names=['On-time', 'Late']))"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(is_late_true, is_late_pred)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['On-time', 'Late'])\n",
        "disp.plot(ax=ax, cmap='Blues', colorbar=False)\n",
        "plt.title('Confusion Matrix - Late Delivery Prediction')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# STEP 3: Improving the Standard Solution\n",
        "---\n",
        "\n",
        "## 3.1 Hyperparameter Tuning with Cross-Validation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validation setup\n",
        "print(\"=\"*60)\n",
        "print(\"CROSS-VALIDATION BASELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Preprocess data for cross-validation\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "print(f\"Processed training shape: {X_train_processed.shape}\")\n",
        "print(f\"Processed test shape: {X_test_processed.shape}\")\n",
        "\n",
        "# Cross-validation scores for baseline (3-fold for speed)\n",
        "cv_model = HistGradientBoostingRegressor(max_iter=150, random_state=42)\n",
        "cv_scores = cross_val_score(cv_model, X_train_processed, y_train_reg, \n",
        "                            cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "\n",
        "print(f\"\\nCross-validation MAE scores: {-cv_scores}\")\n",
        "print(f\"Mean CV MAE: {-cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning with GridSearchCV (optimized for speed)\n",
        "print(\"=\"*60)\n",
        "print(\"HYPERPARAMETER TUNING - GridSearchCV\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Reduced parameter grid for faster execution\n",
        "# Full grid would take ~15-20 min, this takes ~3-5 min\n",
        "param_grid = {\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'max_leaf_nodes': [31, 63],\n",
        "    'min_samples_leaf': [50, 100],\n",
        "}\n",
        "\n",
        "# GridSearchCV with parallel processing\n",
        "grid_search = GridSearchCV(\n",
        "    HistGradientBoostingRegressor(max_iter=200, early_stopping=True, \n",
        "                                   validation_fraction=0.1, random_state=42),\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='neg_mean_absolute_error',\n",
        "    n_jobs=-1,  # Use all CPU cores\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Running GridSearchCV (this may take 2-4 minutes)...\")\n",
        "print(f\"Testing {len(param_grid['learning_rate'])*len(param_grid['max_leaf_nodes'])*len(param_grid['min_samples_leaf'])*3} model fits...\")\n",
        "\n",
        "grid_search.fit(X_train_processed, y_train_reg)\n",
        "\n",
        "print(f\"\\n\u2705 Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best CV MAE: {-grid_search.best_score_:.3f}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate tuned model\n",
        "print(\"=\"*60)\n",
        "print(\"TUNED MODEL EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_tuned = best_model.predict(X_test_processed)\n",
        "\n",
        "# Handle NaN\n",
        "mask_tuned = ~np.isnan(y_test_reg.values) & ~np.isnan(y_pred_tuned)\n",
        "y_true_tuned = y_test_reg.values[mask_tuned]\n",
        "y_pred_tuned_eval = y_pred_tuned[mask_tuned]\n",
        "\n",
        "mae_tuned = mean_absolute_error(y_true_tuned, y_pred_tuned_eval)\n",
        "rmse_tuned = np.sqrt(mean_squared_error(y_true_tuned, y_pred_tuned_eval))\n",
        "r2_tuned = r2_score(y_true_tuned, y_pred_tuned_eval)\n",
        "\n",
        "print(f\"\\n\u2705 Tuned Model Results:\")\n",
        "print(f\"   MAE:  {mae_tuned:.3f} days (baseline: {mae:.3f})\")\n",
        "print(f\"   RMSE: {rmse_tuned:.3f} days (baseline: {rmse:.3f})\")\n",
        "print(f\"   R\u00b2:   {r2_tuned:.3f} (baseline: {r2:.3f})\")\n",
        "\n",
        "improvement = (mae - mae_tuned) / mae * 100\n",
        "print(f\"\\n   MAE Improvement: {improvement:.1f}%\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Dimension Reduction with PCA"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA Analysis\n",
        "print(\"=\"*60)\n",
        "print(\"DIMENSION REDUCTION - PCA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Apply PCA\n",
        "pca_full = PCA(random_state=42)\n",
        "pca_full.fit(X_train_processed)\n",
        "\n",
        "# Explained variance\n",
        "cumulative_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
        "\n",
        "# Find components for 95% variance\n",
        "n_components_95 = np.argmax(cumulative_var >= 0.95) + 1\n",
        "n_components_90 = np.argmax(cumulative_var >= 0.90) + 1\n",
        "\n",
        "print(f\"\\nTotal features: {X_train_processed.shape[1]}\")\n",
        "print(f\"Components for 90% variance: {n_components_90}\")\n",
        "print(f\"Components for 95% variance: {n_components_95}\")\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Scree plot\n",
        "axes[0].bar(range(1, min(51, len(pca_full.explained_variance_ratio_)+1)), \n",
        "            pca_full.explained_variance_ratio_[:50], alpha=0.8)\n",
        "axes[0].set_xlabel('Principal Component')\n",
        "axes[0].set_ylabel('Explained Variance Ratio')\n",
        "axes[0].set_title('Scree Plot (first 50 components)')\n",
        "\n",
        "# Cumulative variance\n",
        "axes[1].plot(range(1, len(cumulative_var)+1), cumulative_var, 'b-', linewidth=2)\n",
        "axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
        "axes[1].axhline(y=0.90, color='orange', linestyle='--', label='90% threshold')\n",
        "axes[1].axvline(x=n_components_95, color='r', linestyle=':', alpha=0.5)\n",
        "axes[1].set_xlabel('Number of Components')\n",
        "axes[1].set_ylabel('Cumulative Explained Variance')\n",
        "axes[1].set_title('Cumulative Explained Variance')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with PCA (optimized)\n",
        "print(\"=\"*60)\n",
        "print(\"MODEL WITH PCA REDUCTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Apply PCA with 95% variance\n",
        "pca = PCA(n_components=0.95, random_state=42)\n",
        "X_train_pca = pca.fit_transform(X_train_processed)\n",
        "X_test_pca = pca.transform(X_test_processed)\n",
        "\n",
        "print(f\"\\nReduced dimensions: {X_train_processed.shape[1]} \u2192 {X_train_pca.shape[1]}\")\n",
        "\n",
        "# Train model on reduced data (optimized)\n",
        "model_pca = HistGradientBoostingRegressor(\n",
        "    max_iter=150,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "model_pca.fit(X_train_pca, y_train_reg)\n",
        "y_pred_pca = model_pca.predict(X_test_pca)\n",
        "\n",
        "# Evaluate\n",
        "mask_pca = ~np.isnan(y_test_reg.values) & ~np.isnan(y_pred_pca)\n",
        "mae_pca = mean_absolute_error(y_test_reg.values[mask_pca], y_pred_pca[mask_pca])\n",
        "rmse_pca = np.sqrt(mean_squared_error(y_test_reg.values[mask_pca], y_pred_pca[mask_pca]))\n",
        "\n",
        "print(f\"\\n\u2705 PCA Model Results:\")\n",
        "print(f\"   MAE:  {mae_pca:.3f} days\")\n",
        "print(f\"   RMSE: {rmse_pca:.3f} days\")\n",
        "print(f\"   Dimension reduction: {(1 - X_train_pca.shape[1]/X_train_processed.shape[1])*100:.1f}%\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Multiple Models Comparison"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare multiple models (optimized for speed)\n",
        "print(\"=\"*60)\n",
        "print(\"MULTIPLE MODELS COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define models to compare (reduced complexity for speed)\n",
        "models = {\n",
        "    'Ridge Regression': Ridge(alpha=1.0),\n",
        "    'Random Forest': RandomForestRegressor(\n",
        "        n_estimators=50,        # Reduced from 100\n",
        "        max_depth=8,            # Reduced from 10\n",
        "        min_samples_leaf=50, \n",
        "        random_state=42, \n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(\n",
        "        n_estimators=50,        # Reduced from 100\n",
        "        max_depth=4,            # Reduced from 5\n",
        "        learning_rate=0.1, \n",
        "        random_state=42\n",
        "    ),\n",
        "    'HistGradientBoosting': HistGradientBoostingRegressor(\n",
        "        max_iter=150,           # Reduced\n",
        "        learning_rate=0.1,\n",
        "        random_state=42\n",
        "    ),\n",
        "    'XGBoost': xgb.XGBRegressor(\n",
        "        n_estimators=50,        # Reduced from 100\n",
        "        max_depth=4,            # Reduced from 5\n",
        "        learning_rate=0.1,\n",
        "        random_state=42, \n",
        "        n_jobs=-1\n",
        "    )\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    \n",
        "    # Cross-validation (3-fold for speed)\n",
        "    cv_scores = cross_val_score(model, X_train_processed, y_train_reg, \n",
        "                                cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "    \n",
        "    # Fit and predict\n",
        "    model.fit(X_train_processed, y_train_reg)\n",
        "    y_pred = model.predict(X_test_processed)\n",
        "    \n",
        "    # Compute metrics\n",
        "    mask = ~np.isnan(y_test_reg.values) & ~np.isnan(y_pred)\n",
        "    mae_val = mean_absolute_error(y_test_reg.values[mask], y_pred[mask])\n",
        "    rmse_val = np.sqrt(mean_squared_error(y_test_reg.values[mask], y_pred[mask]))\n",
        "    r2_val = r2_score(y_test_reg.values[mask], y_pred[mask])\n",
        "    \n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'CV MAE': -cv_scores.mean(),\n",
        "        'CV Std': cv_scores.std(),\n",
        "        'Test MAE': mae_val,\n",
        "        'Test RMSE': rmse_val,\n",
        "        'Test R\u00b2': r2_val\n",
        "    })\n",
        "\n",
        "# Display results\n",
        "results_df = pd.DataFrame(results).sort_values('Test MAE')\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON RESULTS\")\n",
        "print(\"=\"*60)\n",
        "display(results_df.round(4))"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize model comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# MAE comparison\n",
        "colors = plt.cm.viridis(np.linspace(0, 0.8, len(results_df)))\n",
        "bars = axes[0].barh(results_df['Model'], results_df['Test MAE'], color=colors)\n",
        "axes[0].set_xlabel('Test MAE (days)')\n",
        "axes[0].set_title('Model Comparison - MAE')\n",
        "axes[0].invert_yaxis()\n",
        "for bar, val in zip(bars, results_df['Test MAE']):\n",
        "    axes[0].text(val + 0.01, bar.get_y() + bar.get_height()/2, \n",
        "                 f'{val:.3f}', va='center', fontsize=10)\n",
        "\n",
        "# R\u00b2 comparison\n",
        "bars = axes[1].barh(results_df['Model'], results_df['Test R\u00b2'], color=colors)\n",
        "axes[1].set_xlabel('Test R\u00b2')\n",
        "axes[1].set_title('Model Comparison - R\u00b2')\n",
        "axes[1].invert_yaxis()\n",
        "for bar, val in zip(bars, results_df['Test R\u00b2']):\n",
        "    axes[1].text(val + 0.01, bar.get_y() + bar.get_height()/2, \n",
        "                 f'{val:.3f}', va='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Ensemble Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensemble model - Voting Regressor (optimized)\n",
        "print(\"=\"*60)\n",
        "print(\"ENSEMBLE MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "\n",
        "# Create ensemble with best models (reduced complexity)\n",
        "ensemble = VotingRegressor([\n",
        "    ('hgb', HistGradientBoostingRegressor(\n",
        "        max_iter=150, \n",
        "        learning_rate=0.1,\n",
        "        random_state=42\n",
        "    )),\n",
        "    ('xgb', xgb.XGBRegressor(\n",
        "        n_estimators=50, \n",
        "        max_depth=4, \n",
        "        learning_rate=0.1, \n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )),\n",
        "    ('rf', RandomForestRegressor(\n",
        "        n_estimators=50, \n",
        "        max_depth=8,\n",
        "        min_samples_leaf=50, \n",
        "        random_state=42, \n",
        "        n_jobs=-1\n",
        "    ))\n",
        "], n_jobs=-1)\n",
        "\n",
        "print(\"Training ensemble model...\")\n",
        "ensemble.fit(X_train_processed, y_train_reg)\n",
        "y_pred_ensemble = ensemble.predict(X_test_processed)\n",
        "\n",
        "# Evaluate\n",
        "mask_ens = ~np.isnan(y_test_reg.values) & ~np.isnan(y_pred_ensemble)\n",
        "mae_ens = mean_absolute_error(y_test_reg.values[mask_ens], y_pred_ensemble[mask_ens])\n",
        "rmse_ens = np.sqrt(mean_squared_error(y_test_reg.values[mask_ens], y_pred_ensemble[mask_ens]))\n",
        "r2_ens = r2_score(y_test_reg.values[mask_ens], y_pred_ensemble[mask_ens])\n",
        "\n",
        "print(f\"\\n\u2705 Ensemble Model Results:\")\n",
        "print(f\"   MAE:  {mae_ens:.3f} days\")\n",
        "print(f\"   RMSE: {rmse_ens:.3f} days\")\n",
        "print(f\"   R\u00b2:   {r2_ens:.3f}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Advanced Model - XGBoost with Full Tuning"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced XGBoost tuning (optimized)\n",
        "print(\"=\"*60)\n",
        "print(\"ADVANCED MODEL - XGBoost Tuning\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Reduced parameter grid for faster execution\n",
        "xgb_param_grid = {\n",
        "    'n_estimators': [100, 150],\n",
        "    'max_depth': [4, 6],\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'subsample': [0.8],\n",
        "    'colsample_bytree': [0.8]\n",
        "}\n",
        "\n",
        "xgb_model = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
        "\n",
        "xgb_search = RandomizedSearchCV(\n",
        "    xgb_model,\n",
        "    xgb_param_grid,\n",
        "    n_iter=10,  # Reduced iterations\n",
        "    cv=3,\n",
        "    scoring='neg_mean_absolute_error',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Running XGBoost RandomizedSearchCV (2-3 minutes)...\")\n",
        "xgb_search.fit(X_train_processed, y_train_reg)\n",
        "\n",
        "print(f\"\\n\u2705 Best XGBoost parameters: {xgb_search.best_params_}\")\n",
        "print(f\"Best CV MAE: {-xgb_search.best_score_:.3f}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final XGBoost evaluation\n",
        "best_xgb = xgb_search.best_estimator_\n",
        "y_pred_xgb = best_xgb.predict(X_test_processed)\n",
        "\n",
        "mask_xgb = ~np.isnan(y_test_reg.values) & ~np.isnan(y_pred_xgb)\n",
        "mae_xgb = mean_absolute_error(y_test_reg.values[mask_xgb], y_pred_xgb[mask_xgb])\n",
        "rmse_xgb = np.sqrt(mean_squared_error(y_test_reg.values[mask_xgb], y_pred_xgb[mask_xgb]))\n",
        "r2_xgb = r2_score(y_test_reg.values[mask_xgb], y_pred_xgb[mask_xgb])\n",
        "\n",
        "print(f\"\\n\u2705 Tuned XGBoost Results:\")\n",
        "print(f\"   MAE:  {mae_xgb:.3f} days\")\n",
        "print(f\"   RMSE: {rmse_xgb:.3f} days\")\n",
        "print(f\"   R\u00b2:   {r2_xgb:.3f}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance from XGBoost\n",
        "print(\"=\"*60)\n",
        "print(\"FEATURE IMPORTANCE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get feature names after preprocessing\n",
        "try:\n",
        "    feature_names = (num_features + \n",
        "                    list(preprocessor.named_transformers_['cat']\n",
        "                         .named_steps['encoder'].get_feature_names_out(cat_features)))\n",
        "except:\n",
        "    feature_names = [f'feature_{i}' for i in range(X_train_processed.shape[1])]\n",
        "\n",
        "# XGBoost feature importance\n",
        "importance = best_xgb.feature_importances_\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_names[:len(importance)],\n",
        "    'importance': importance\n",
        "}).sort_values('importance', ascending=False).head(20)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(importance_df['feature'], importance_df['importance'], color='steelblue')\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Top 20 Feature Importances (XGBoost)')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 Final Classification Evaluation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification from best regression model\n",
        "print(\"=\"*60)\n",
        "print(\"FINAL CLASSIFICATION EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Use ensemble predictions for classification\n",
        "scheduled_test_full = df_clean.loc[X_test.index, scheduled_col].values\n",
        "\n",
        "# Align with mask\n",
        "scheduled_aligned = scheduled_test_full[mask_ens]\n",
        "y_true_class_final = (y_test_reg.values[mask_ens] > scheduled_aligned).astype(int)\n",
        "y_pred_class_final = (y_pred_ensemble[mask_ens] > scheduled_aligned).astype(int)\n",
        "\n",
        "# Classification metrics\n",
        "print(\"\\nClassification Report (Ensemble \u2192 Late Prediction):\")\n",
        "print(classification_report(y_true_class_final, y_pred_class_final, \n",
        "                            target_names=['On-time', 'Late']))\n",
        "\n",
        "# Confusion matrix\n",
        "cm_final = confusion_matrix(y_true_class_final, y_pred_class_final)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_final, display_labels=['On-time', 'Late'])\n",
        "disp.plot(ax=ax, cmap='Blues', colorbar=False)\n",
        "plt.title('Final Confusion Matrix - Ensemble Model')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Final Summary & Conclusion\n",
        "---"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Final summary\n",
        "print(\"=\"*70)\n",
        "print(\"                    FINAL MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "summary_data = [\n",
        "    ['Baseline (HistGB)', mae, rmse, r2],\n",
        "    ['Tuned HistGB', mae_tuned, rmse_tuned, r2_tuned],\n",
        "    ['With PCA', mae_pca, rmse_pca, '-'],\n",
        "    ['Ensemble', mae_ens, rmse_ens, r2_ens],\n",
        "    ['Tuned XGBoost', mae_xgb, rmse_xgb, r2_xgb]\n",
        "]\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data, columns=['Model', 'MAE', 'RMSE', 'R\u00b2'])\n",
        "display(summary_df)\n",
        "\n",
        "# Find best model\n",
        "best_mae_idx = summary_df['MAE'].astype(float).idxmin()\n",
        "best_model_name = summary_df.loc[best_mae_idx, 'Model']\n",
        "best_mae_val = summary_df.loc[best_mae_idx, 'MAE']\n",
        "\n",
        "print(f\"\\n\ud83c\udfc6 BEST MODEL: {best_model_name}\")\n",
        "print(f\"   Best MAE: {best_mae_val:.3f} days\")\n",
        "\n",
        "# Improvement over baseline\n",
        "improvement_final = (mae - float(best_mae_val)) / mae * 100\n",
        "print(f\"   Improvement over baseline: {improvement_final:.1f}%\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conclusion\n",
        "print(\"=\"*70)\n",
        "print(\"                           CONCLUSION\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "BUSINESS CASE RESOLUTION:\n",
        "\n",
        "1. PROBLEM: Predict real shipping time to identify late deliveries in advance.\n",
        "\n",
        "2. APPROACH:\n",
        "   - Regression to predict exact shipping days\n",
        "   - Classification derived from regression for late/on-time prediction\n",
        "   - Multiple models compared: Ridge, Random Forest, Gradient Boosting, XGBoost\n",
        "   - Ensemble methods for improved robustness\n",
        "\n",
        "3. KEY FINDINGS:\n",
        "   - Best MAE achieved: ~1.0 day prediction error\n",
        "   - Model can identify late deliveries with ~65% precision and ~79% recall\n",
        "   - Days for shipment (scheduled) is the most important predictor\n",
        "   - Shipping Mode and Order Region also significant\n",
        "\n",
        "4. RECOMMENDATIONS:\n",
        "   - Deploy ensemble model for production predictions\n",
        "   - Set up alerts for orders predicted as late\n",
        "   - Monitor model performance and retrain periodically\n",
        "   - Consider collecting additional features (warehouse location, weather)\n",
        "\n",
        "5. OBSTACLES ADDRESSED:\n",
        "   - Data leakage prevention (excluded post-delivery columns)\n",
        "   - Group-aware train/test split (Order Id)\n",
        "   - Missing values (imputation)\n",
        "   - High cardinality categoricals (min_frequency encoding)\n",
        "   - Overfitting (cross-validation, regularization, early stopping)\n",
        "\"\"\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# References\n",
        "\n",
        "1. **XGBoost**: Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. *KDD '16*.\n",
        "\n",
        "2. **Gradient Boosting**: Friedman, J. H. (2001). Greedy Function Approximation: A Gradient Boosting Machine. *Annals of Statistics*.\n",
        "\n",
        "3. **Random Forest**: Breiman, L. (2001). Random Forests. *Machine Learning*, 45(1), 5-32.\n",
        "\n",
        "4. **Scikit-learn**: Pedregosa et al. (2011). Scikit-learn: Machine Learning in Python. *JMLR* 12, pp. 2825-2830.\n",
        "\n",
        "5. **Supply Chain Analytics**: Various Kaggle notebooks and academic papers on supply chain prediction.\n",
        "\n",
        "---"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# \ud83d\udce4 Export to HTML & Save\n",
        "\n",
        "Run the cell below **AFTER** executing all other cells to generate:\n",
        "1. **Clean HTML** (results only, no code) - perfect for presentation\n",
        "2. **Full HTML** (with code) - for technical review\n",
        "\n",
        "Both files will be saved to your Google Drive automatically."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# EXPORT TO HTML - Run this cell LAST (after all other cells)\n",
        "# ============================================================\n",
        "# This generates a high-quality HTML report with all outputs\n",
        "\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "\n",
        "# Notebook name\n",
        "notebook_name = 'RenduML_Cyprien'\n",
        "\n",
        "# Generate HTML with embedded images (high quality)\n",
        "print(\"\ud83d\udcc4 Generating HTML report...\")\n",
        "!jupyter nbconvert --to html /content/{notebook_name}.ipynb \\\n",
        "    --output /content/{notebook_name}.html \\\n",
        "    --HTMLExporter.theme=dark \\\n",
        "    --no-input\n",
        "\n",
        "# Also generate a version WITH code visible\n",
        "!jupyter nbconvert --to html /content/{notebook_name}.ipynb \\\n",
        "    --output /content/{notebook_name}_with_code.html\n",
        "\n",
        "# Copy to Google Drive for easy access\n",
        "!cp /content/{notebook_name}.html /content/drive/MyDrive/{notebook_name}.html\n",
        "!cp /content/{notebook_name}_with_code.html /content/drive/MyDrive/{notebook_name}_with_code.html\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\u2705 HTML EXPORT COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\"\"\n",
        "\ud83d\udcc1 FILES GENERATED:\n",
        "\n",
        "1. {notebook_name}.html (clean version - no code, just results)\n",
        "   \u2192 /content/{notebook_name}.html\n",
        "   \u2192 /content/drive/MyDrive/{notebook_name}.html\n",
        "\n",
        "2. {notebook_name}_with_code.html (full version with code)\n",
        "   \u2192 /content/{notebook_name}_with_code.html  \n",
        "   \u2192 /content/drive/MyDrive/{notebook_name}_with_code.html\n",
        "\n",
        "\ud83d\udca1 HOW TO DOWNLOAD:\n",
        "   - Click the \ud83d\udcc1 folder icon on the left panel\n",
        "   - Navigate to the file\n",
        "   - Right-click \u2192 Download\n",
        "   \n",
        "   OR find them in your Google Drive!\n",
        "\n",
        "\ud83d\udce4 TO PUSH TO GITHUB MANUALLY:\n",
        "   1. Download the .ipynb file from Colab (File \u2192 Download \u2192 .ipynb)\n",
        "   2. Go to: https://github.com/victorbarthvby/Machinelearning_project_A4\n",
        "   3. Click \"Add file\" \u2192 \"Upload files\"\n",
        "   4. Drag & drop your notebook\n",
        "   5. Commit changes\n",
        "\"\"\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}